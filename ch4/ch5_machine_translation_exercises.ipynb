{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine translation\n",
    "\n",
    "In this exercise, we will look at how to perform machine translation with sequence to sequence models.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "Let's first load the data. We will be using data from [this Udacity repo]() that has a set of English and French sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "n_rows = 20000\n",
    "en_text = []\n",
    "with open('ch3.small_vocab_en.txt', 'r', encoding='utf-8') as f:\n",
    "    for ri,row in enumerate(f):\n",
    "        en_text.append(row.strip())\n",
    "        if ri>=n_rows-1: break\n",
    "            \n",
    "fr_text = []\n",
    "with open('ch3.small_vocab_fr.txt', 'r', encoding='utf-8') as f:\n",
    "    for ri, row in enumerate(f):\n",
    "        fr_text.append(row.strip())\n",
    "        if ri>=n_rows-1: break\n",
    "            \n",
    "en_ser = pd.Series(en_text)\n",
    "fr_ser = pd.Series(fr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing some data\n",
    "\n",
    "We will print some data and see if they align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\tFrench:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril . \n",
      "\n",
      "English:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\tFrench:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . \n",
      "\n",
      "English:  california is usually quiet during march , and it is usually hot in june .\n",
      "\tFrench:  california est généralement calme en mars , et il est généralement chaud en juin . \n",
      "\n",
      "English:  the united states is sometimes mild during june , and it is cold in september .\n",
      "\tFrench:  les états-unis est parfois légère en juin , et il fait froid en septembre . \n",
      "\n",
      "English:  your least liked fruit is the grape , but my least liked is the apple .\n",
      "\tFrench:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,(en,fr) in enumerate(zip(en_text[:5], fr_text[:5])):\n",
    "    print('English: ', en)\n",
    "    print('\\tFrench: ',fr,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing French sentences \n",
    "\n",
    "The main preprocessing step that will be performed is appending `<sos>` (beginning of the sentence) and `<eos>` (end of the sentence) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_ser = fr_ser.apply(lambda x: ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data to train, valid and test\n",
    "\n",
    "Let's now split the data to train/valid/test data. What's the purpose of each set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (12000, 2)\n",
      "Valid shape: (4000, 2)\n",
      "Test shape: (4000, 2)\n",
      "['my least liked fruit is the lime , but her least liked is the orange .', 'you dislike oranges , apples , and grapefruit .']\n",
      "[\"<sos> mon fruit est moins aimé la chaux , mais elle est moins aimé l'orange . <eos>\", \"<sos> vous n'aimez pas les oranges , les pommes et le pamplemousse . <eos>\"]\n",
      "\n",
      "\n",
      "['new jersey is usually freezing during october , but it is never nice in july .', 'france is usually dry during spring , but it is wonderful in november .']\n",
      "['<sos> new jersey est le gel habituellement en octobre , mais il est jamais agréable en juillet . <eos>', '<sos> la france est généralement sec au printemps , mais il est merveilleux en novembre . <eos>']\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([en_ser, fr_ser], axis=1)\n",
    "df = df.rename(columns=dict(zip(df.columns.tolist(),['en','fr'])))\n",
    "df = df.sample(frac=1.0, random_state=100) # Shuffling data as a precaution\n",
    "\n",
    "# Splitting data to train/valid/test\n",
    "train_df = df.sample(frac=0.6, random_state=100)\n",
    "test_valid_df = df.drop(train_df.index)\n",
    "valid_df = test_valid_df.sample(frac=0.5, random_state=100)\n",
    "test_df = test_valid_df.drop(valid_df.index)\n",
    "del test_valid_df\n",
    "\n",
    "# Printing shapes\n",
    "print(\"Train shape: {}\".format(train_df.shape))\n",
    "print(\"Valid shape: {}\".format(valid_df.shape))\n",
    "print(\"Test shape: {}\".format(test_df.shape))\n",
    "\n",
    "# Converting dataframes to lists\n",
    "train_en_text = train_df[\"en\"].tolist()\n",
    "train_fr_text = train_df[\"fr\"].tolist()\n",
    "valid_en_text = valid_df[\"en\"].tolist()\n",
    "valid_fr_text = valid_df[\"fr\"].tolist()\n",
    "test_en_text = test_df[\"en\"].tolist()\n",
    "test_fr_text = test_df[\"fr\"].tolist()\n",
    "\n",
    "print(train_en_text[:2])\n",
    "print(train_fr_text[:2])\n",
    "print('\\n')\n",
    "print(test_en_text[:2])\n",
    "print(test_fr_text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a tokenizer and preprocessing\n",
    "\n",
    "We will fit two tokenizers. One for the English corpus and the other for the French corpus. We will also write a function called `preprocess_text(...)` that does basic preprocessing of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 100, 250)\n",
      "(12000, 100, 250)\n"
     ]
    }
   ],
   "source": [
    "n_en_vocab, n_fr_vocab = 250, 250\n",
    "en_len, fr_len = 100, 100\n",
    "def create_and_fit_tokenizer(text, n_vocab):\n",
    "    # Return a tokenizer (tok) with n_vocab words and <unk> OOV token\n",
    "    # Fit on text\n",
    "    return tok\n",
    "\n",
    "en_tok = create_and_fit_tokenizer(train_en_text, n_en_vocab)\n",
    "fr_tok = create_and_fit_tokenizer(train_fr_text, n_fr_vocab)\n",
    "\n",
    "def preprocess_text(tok, text, pad_len, pad_type, truncate_type, n_vocab, onehot=False):\n",
    "    seq = tok.texts_to_sequences(text)\n",
    "    # PAd the sequence using the correct arguments\n",
    "    pad_seq = ____\n",
    "    if onehot:\n",
    "        # Convert padded sequence to onehot encoded vectors using to_categorical\n",
    "        pad_seq = ____\n",
    "    return pad_seq\n",
    "\n",
    "train_x = preprocess_text(en_tok, train_en_text, en_len, 'pre', 'post', n_en_vocab, onehot=True)\n",
    "valid_x = preprocess_text(en_tok, valid_en_text, en_len, 'pre', 'post', n_en_vocab, onehot=True)\n",
    "print(train_x.shape)\n",
    "train_y = preprocess_text(fr_tok, train_fr_text, fr_len, 'post', 'post', n_fr_vocab, onehot=True)\n",
    "valid_y = preprocess_text(fr_tok, valid_fr_text, fr_len, 'post', 'post', n_fr_vocab, onehot=True)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# Without these lines I get \n",
    "# > UnknownError:  [_Derived_]  Fail to find the dnn implementation.\n",
    "# >  [[{{node CudnnRNN}}]]\n",
    "# >  [[model/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_5819]\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Keras model\n",
    "\n",
    "Let's now define a simple sequence to sequence model. This model has the following.\n",
    "\n",
    "* Bidirectional LSTM layer with 50 nodes\n",
    "* A repeat vector layer\n",
    "* A decoder LSTM layer with 50 nodes\n",
    "* A TimeDistributed Dense layer with `n_fr_vocab` nodes\n",
    "\n",
    "We will use `categorical_crossentropy` as the loss and `adam` as the optimizer. Furthermore, we'll use `acc` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 250)]        0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 50)           30200     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 250)          12750     \n",
      "=================================================================\n",
      "Total params: 163,350\n",
      "Trainable params: 163,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write the encoder\n",
    "____\n",
    "____\n",
    "\n",
    "# Write the decoder\n",
    "____\n",
    "____\n",
    "____\n",
    "\n",
    "# Define the model\n",
    "model = ____(____, ____)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Let's now train the model for 10 epochs while using validation data to monitor the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 24s 2ms/sample - loss: 0.9960 - acc: 0.8586 - val_loss: 0.5826 - val_acc: 0.8776\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.5514 - acc: 0.8823 - val_loss: 0.5152 - val_acc: 0.8874\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.4863 - acc: 0.8896 - val_loss: 0.4494 - val_acc: 0.8930\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.4282 - acc: 0.8971 - val_loss: 0.4052 - val_acc: 0.9023\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.3950 - acc: 0.9019 - val_loss: 0.3786 - val_acc: 0.9046\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.3762 - acc: 0.9027 - val_loss: 0.3623 - val_acc: 0.9063\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.3583 - acc: 0.9054 - val_loss: 0.3481 - val_acc: 0.9072\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.3473 - acc: 0.9065 - val_loss: 0.3448 - val_acc: 0.9072\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.3362 - acc: 0.9087 - val_loss: 0.3308 - val_acc: 0.9092\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.3269 - acc: 0.9106 - val_loss: 0.3212 - val_acc: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x209bee691d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on training data\n",
    "____(____, ____, epochs=10, validation_data=(____, ____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Here we will evaluate the model on the test data and compute the test loss and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.32266148686408996 & test accuracy: 0.911454975605011\n"
     ]
    }
   ],
   "source": [
    "# Freeing up memory\n",
    "try:\n",
    "    del train_en_text, train_fr_text\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "test_x = preprocess_text(en_tok, test_en_text, en_len, 'pre', 'post', n_en_vocab, onehot=True)\n",
    "test_y = preprocess_text(fr_tok, test_fr_text, fr_len, 'post', 'post', n_fr_vocab, onehot=True)\n",
    "\n",
    "res = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Test loss: {} & test accuracy: {}\".format(res[0], res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model with teacher forcing\n",
    "\n",
    "This new model has two input layers. One for the encoder another for the decoder.\n",
    "\n",
    "* Bidirectional GRU layer with 50 nodes\n",
    "* A decoder GRU layer with 50 nodes\n",
    "* A TimeDistributed Dense layer with `n_fr_vocab` nodes\n",
    "\n",
    "We will use `categorical_crossentropy` as the loss and `adam` as the optimizer. Furthermore, we'll use `acc` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 100, 250)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 99, 250)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 100), (None, 90600       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 99, 50)       45300       input_5[0][0]                    \n",
      "                                                                 bidirectional_2[0][2]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 99, 250)      12750       gru_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 148,650\n",
      "Trainable params: 148,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# Encoder\n",
    "enc_inp = layers.Input(shape=(en_len, n_en_vocab))\n",
    "enc_out, enc_fwd, enc_fwd = layers.Bidirectional(layers.GRU(50, return_state=True))(enc_inp)\n",
    "# Decoder\n",
    "dec_inp = layers.Input(shape=(fr_len-1, n_fr_vocab))\n",
    "# Define the rest of the decoder\n",
    "____\n",
    "____\n",
    "\n",
    "model2 = models.Model(inputs=[enc_inp, dec_inp], outputs=dec_out)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bidirectional_2/forward_gru_2/kernel:0', 'bidirectional_2/forward_gru_2/recurrent_kernel:0', 'bidirectional_2/forward_gru_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bidirectional_2/forward_gru_2/kernel:0', 'bidirectional_2/forward_gru_2/recurrent_kernel:0', 'bidirectional_2/forward_gru_2/bias:0'] when minimizing the loss.\n",
      "12000/12000 [==============================] - 19s 2ms/sample - loss: 1.0980 - acc: 0.8601 - val_loss: 0.5088 - val_acc: 0.8931\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.4306 - acc: 0.9085 - val_loss: 0.3493 - val_acc: 0.9196\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 16s 1ms/sample - loss: 0.2991 - acc: 0.9282 - val_loss: 0.2610 - val_acc: 0.9340\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.2397 - acc: 0.9355 - val_loss: 0.2241 - val_acc: 0.9372\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.2129 - acc: 0.9377 - val_loss: 0.2057 - val_acc: 0.9383\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 16s 1ms/sample - loss: 0.1980 - acc: 0.9397 - val_loss: 0.1939 - val_acc: 0.9399\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.1880 - acc: 0.9413 - val_loss: 0.1855 - val_acc: 0.9414\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.1809 - acc: 0.9421 - val_loss: 0.1795 - val_acc: 0.9422\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.1758 - acc: 0.9428 - val_loss: 0.1755 - val_acc: 0.9429\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 15s 1ms/sample - loss: 0.1719 - acc: 0.9433 - val_loss: 0.1720 - val_acc: 0.9429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x208ca983ef0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correctly fill the data that needs to be passed to model2\n",
    "model2.fit(\n",
    "    [____, train_y[____]], train_y[____], epochs=10, \n",
    "    validation_data=(____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Model 2) Test loss: 0.1721156494617462 & test accuracy: 0.942570686340332\n"
     ]
    }
   ],
   "source": [
    "res = model2.evaluate([test_x, test_y[:,:-1,:]], test_y[:,1:,:], verbose=0)\n",
    "print(\"(Model 2) Test loss: {} & test accuracy: {}\".format(res[0], res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
